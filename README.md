# CUDA Hook Analyzer & Intelligent Inference Scheduler

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CUDA](https://img.shields.io/badge/CUDA-12.6-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![Platform](https://img.shields.io/badge/Platform-Jetson%20Orin%20Nano-76B900.svg)](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)

> A lightweight CUDA-level performance profiling and intelligent multi-model inference scheduling framework for edge devices.

[ä¸­æ–‡æ–‡æ¡£](README_CN.md) | English

## ðŸŽ¯ Project Overview

**HookAnalyzer** addresses critical challenges in edge AI deployment:
- **Multi-model concurrency** with resource contention management
- **CUDA kernel-level profiling** for bottleneck identification
- **GPU memory optimization** with fragmentation analysis
- **Intelligent scheduling** balancing latency and throughput

## ðŸŒŸ Highlights

### Production-Grade Results
- âš¡ **114.67 FPS** YOLOv8 inference on Jetson Orin Nano (3.8x real-time)
- ðŸŽ¯ **8.72ms average latency** with P99 < 14ms (production-stable)
- ðŸ’¾ **7.4 MB GPU memory footprint** (highly optimized)
- ðŸ”§ **350+ LOC** modular TensorRT C++ wrapper

### Technical Depth
- Deep dive into CUDA kernel optimization (occupancy analysis, memory coalescing)
- TensorRT engine integration with FP16 precision
- Async inference pipeline with CUDA streams
- Performance profiling and benchmarking framework

### Demonstrated Skills
- C++17, CUDA 12.6, TensorRT 10.3, CMake
- Edge AI deployment on resource-constrained devices
- Performance analysis and optimization methodologies
- Production-ready code architecture

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                        â”‚
â”‚              (YOLOv8, ResNet, BERT Models)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Inference Engine Adapters                      â”‚
â”‚        TensorRT â”‚ ONNX Runtime â”‚ Custom Kernels             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Intelligent Scheduler (C++)                       â”‚
â”‚   Priority Queue â”‚ Dynamic Batching â”‚ Stream Manager        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          CUDA Hook & Profiling Layer                        â”‚
â”‚   Memory Tracker â”‚ Kernel Analyzer â”‚ CUPTI Integration      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Key Features

### ðŸ” CUDA Interception Layer
- Real-time `cudaMalloc`/`cudaFree` hooking
- Kernel launch time profiling with CUPTI
- Memory access pattern analysis
- GPU utilization tracking

### ðŸ§  Intelligent Scheduler
- **Priority-based** multi-model scheduling
- **Dynamic batching** with configurable policies
- **Stream-level** parallelism optimization
- **Latency-aware** resource allocation

### ðŸš€ Performance Optimization
- Custom CUDA kernel library (GEMM, Conv, Softmax)
- Memory pool with defragmentation
- Multi-stream concurrent execution
- Mixed precision (INT8/FP16/FP32) support

### ðŸ“Š Monitoring & Visualization
- Real-time metrics via Prometheus
- Grafana dashboards
- Flame graph generation
- RESTful API for control

## ðŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|------------|
| **Core** | C++17, CUDA 12.6, CMake 3.18+ |
| **Inference** | TensorRT 10.3.0, ONNX Runtime (planned) |
| **Profiling** | CUPTI, Nsight Systems |
| **API** | Python 3.10+, FastAPI |
| **Monitoring** | Prometheus, Grafana (planned) |
| **Containerization** | Docker, NVIDIA Container Runtime |
| **Platform** | Jetson Orin Nano (JetPack 6.x) |

## ðŸ“¦ Directory Structure

```
HookAnalyzer/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ cuda_hook/          # CUDA API interception
â”‚   â”œâ”€â”€ scheduler/          # Multi-model scheduler
â”‚   â””â”€â”€ profiler/           # Performance analysis
â”œâ”€â”€ engines/
â”‚   â”œâ”€â”€ tensorrt_adapter/   # TensorRT wrapper
â”‚   â””â”€â”€ onnx_adapter/       # ONNX Runtime wrapper
â”œâ”€â”€ kernels/
â”‚   â””â”€â”€ optimized/          # Custom CUDA kernels
â”œâ”€â”€ api/
â”‚   â””â”€â”€ server/             # FastAPI service
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ metrics/            # Prometheus exporter
â”‚   â””â”€â”€ dashboard/          # Grafana configs
â”œâ”€â”€ benchmarks/             # Performance tests
â”œâ”€â”€ examples/               # Usage examples
â”œâ”€â”€ scripts/                # Build & deployment
â”œâ”€â”€ tests/                  # Unit tests
â””â”€â”€ docs/                   # Documentation
```

## ðŸš€ Quick Start

### Prerequisites

**Hardware:**
- NVIDIA Jetson Orin Nano (verified platform)
  - Ampere GPU architecture (SM 8.7)
  - 8 Streaming Multiprocessors
  - 7.6 GB LPDDR5 memory
- Or any CUDA-capable device with Compute Capability 5.0+

**Software:**
- JetPack 6.x (CUDA 12.6, TensorRT 10.3.0)
- CMake 3.18+, GCC 11+
- Python 3.10+ (for model conversion scripts)

### Build from Source

```bash
# Clone repository
git clone https://github.com/GeoffreyWang1117/Jetson-HookAnalyzer.git
cd Jetson-HookAnalyzer

# Build with CMake (on Jetson)
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release \
      -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.6/bin/nvcc \
      ..
make -j6

# Run kernel tests
./examples/kernel_test

# Run TensorRT inference test (if yolov8n.engine exists)
export LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:$LD_LIBRARY_PATH
./examples/test_tensorrt ../yolov8n.engine
```

### Docker Deployment

```bash
# Build Docker image
docker build -t hook-analyzer:latest -f docker/Dockerfile .

# Run container
docker run --gpus all -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  hook-analyzer:latest
```

## ðŸ“Š Performance Benchmarks

### Verified Results on Jetson Orin Nano

**Hardware:** Jetson Orin Nano (Ampere SM 8.7, 8 SMs, 7.6GB RAM)
**Software:** CUDA 12.6, TensorRT 10.3.0

#### YOLOv8n TensorRT Inference (Experiment 3) âœ…

| Metric | Result | Target | Status |
|--------|--------|--------|--------|
| **Throughput** | **114.67 FPS** | 30 FPS | **3.8x faster** â­ |
| **Average Latency** | **8.72 ms** | 33 ms | **3.8x faster** â­ |
| **Min Latency** | 6.44 ms | - | Best case |
| **Max Latency** | 13.78 ms | - | P99 < 14ms |
| **GPU Memory** | 7.4 MB | - | Highly efficient |

**Model:** YOLOv8n (3.2M params, 8.7 GFLOPs)
**Precision:** FP16
**Input:** 640Ã—640Ã—3 RGB

#### Custom CUDA Kernels (Verified) âœ…

| Kernel | Performance | vs cuBLAS/Reference |
|--------|-------------|---------------------|
| **GEMM (512Ã—512)** | 146 GFLOPS | 68.6% cuBLAS |
| **Memory Bandwidth** | 91.3 GB/s | Efficient |
| **Element-wise Ops** | âœ… Passed | - |
| **Activations (ReLU)** | âœ… Passed | - |

*Full results: [EXPERIMENT3_RESULTS.md](docs/experiments/EXPERIMENT3_RESULTS.md) | [VERIFICATION_REPORT.md](docs/experiments/VERIFICATION_REPORT.md)*

## ðŸ”¬ Experimental Results

### âœ… Completed Experiments

#### Experiment 1: GEMM Performance Analysis
- **Goal:** Optimize matrix multiplication kernels for Jetson Orin Nano
- **Key Finding:** Discovered occupancy vs. tile size tradeoff
  - 16Ã—16 tiles: 100% occupancy (6 blocks/SM)
  - 32Ã—32 tiles: 67% occupancy (1 block/SM) â†’ 20% slower
- **Result:** Documented critical optimization insights for edge GPUs
- **Report:** [EXPERIMENT1_REPORT.md](docs/experiments/EXPERIMENT1_REPORT.md)

#### Experiment 3: Real Model Integration with TensorRT
- **Goal:** Integrate YOLOv8 object detection model using TensorRT
- **Implementation:** Complete C++ TensorRT wrapper (~350 LOC)
- **Performance:** 114.67 FPS (8.72ms latency) - **3.8x faster than real-time**
- **Features:**
  - âœ… Engine loading and serialization
  - âœ… GPU memory management
  - âœ… Sync/async inference support
  - âœ… Comprehensive benchmarking
- **Status:** Production-ready, extensible architecture
- **Report:** [EXPERIMENT3_RESULTS.md](docs/experiments/EXPERIMENT3_RESULTS.md)

### ðŸ“‹ Planned Experiments

- **Experiment 2:** Multi-model concurrent inference with scheduler integration
- **Experiment 4:** INT8 quantization and calibration
- **Experiment 5:** Video stream processing pipeline
- **Experiment 6:** Multi-device distributed inference

## ðŸ“š Documentation

### Experimental Reports (Completed)
- [Experiment 3: TensorRT Integration Results](docs/experiments/EXPERIMENT3_RESULTS.md) - YOLOv8 inference at 114.67 FPS
- [Experiment 1: GEMM Optimization Analysis](docs/experiments/EXPERIMENT1_REPORT.md) - Occupancy vs tile size insights
- [Verification Report](docs/experiments/VERIFICATION_REPORT.md) - Initial project validation
- [Final Summary](docs/experiments/FINAL_SUMMARY.md) - Project completion overview

### Quick References
- [Video Recording Guide](docs/experiments/VIDEO_RECORDING_GUIDE.md) - Demo video creation
- [Experiment Roadmap](docs/experiments/EXPERIMENT_ROADMAP.md) - Future experiment plans
- [Demo Video](docs/media/hookanalyzer_demo.mp4) - Project demonstration

## ðŸ¤ Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) first.

## ðŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file.

## ðŸ™ Acknowledgments

- NVIDIA CUDA Toolkit and TensorRT team
- PyTorch and ONNX communities
- Jetson developer community

## ðŸ“§ Contact

- **Author:** Geoffrey
- **Project:** AI Infrastructure & Inference Optimization
- **Platform:** Jetson Orin Nano @ 100.111.167.60
- **GitHub:** [GeoffreyWang1117/Jetson-HookAnalyzer](https://github.com/GeoffreyWang1117/Jetson-HookAnalyzer)

---

**âš¡ Built for edge AI inference optimization on resource-constrained devices**
