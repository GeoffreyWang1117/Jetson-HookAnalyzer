# HookAnalyzer æ”¹è¿›è·¯çº¿å›¾

åŸºäºé¡¹ç›®å…¨é¢åˆ†æï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åˆ—çš„æ”¹è¿›å»ºè®®ã€‚

## ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³ä¿®å¤ï¼‰

### 1. æ·»åŠ  CUDA é”™è¯¯å¤„ç†

**å—å½±å“æ–‡ä»¶ï¼š**
- `engines/tensorrt_adapter/tensorrt_engine.cpp:107`
- `core/cuda_hook/cuda_hook.cpp:57,237,240`
- `core/scheduler/scheduler.cpp:27-33`

**é—®é¢˜ï¼š**
```cpp
// âŒ é”™è¯¯ç¤ºä¾‹
cudaMalloc(&device_buffers_[i], bytes);  // æ— é”™è¯¯æ£€æŸ¥

// âœ… æ­£ç¡®åšæ³•
cudaError_t err = cudaMalloc(&device_buffers_[i], bytes);
if (err != cudaSuccess) {
    std::cerr << "CUDA allocation failed: " << cudaGetErrorString(err) << std::endl;
    throw std::runtime_error("GPU memory allocation failed");
}
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 2-4 å°æ—¶
**æ”¶ç›Šï¼š** æé«˜ç”Ÿäº§ç¨³å®šæ€§ï¼Œæ˜“äºè°ƒè¯•

---

### 2. å®ç° Python-C++ ç»‘å®š

**å—å½±å“æ–‡ä»¶ï¼š**
- `api/server/main.py` (4 å¤„ TODO)

**å½“å‰çŠ¶æ€ï¼š**
```python
# TODO: Integrate with actual C++ scheduler
output_data = [x * 2.0 for x in request.input_data[:10]]  # å‡æ•°æ®
```

**å®ç°æ–¹æ¡ˆï¼š**

**é€‰é¡¹ Aï¼špybind11ï¼ˆæ¨èï¼‰**
```cpp
// bindings/python_bindings.cpp
#include <pybind11/pybind11.h>
#include "tensorrt_engine.h"

namespace py = pybind11;

PYBIND11_MODULE(hookanalyzer, m) {
    py::class_<TensorRTEngine>(m, "TensorRTEngine")
        .def(py::init<const std::string&>())
        .def("infer", &TensorRTEngine::infer)
        .def("benchmark", &TensorRTEngine::benchmark);
}
```

**é€‰é¡¹ Bï¼šctypesï¼ˆå¿«é€Ÿæ–¹æ¡ˆï¼‰**
```python
import ctypes
lib = ctypes.CDLL('./build/libtensorrt_adapter.so')
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 1-2 å¤©
**æ”¶ç›Šï¼š** API å¯å®é™…ä½¿ç”¨ï¼Œä¸å†æ˜¯æ¼”ç¤º

---

### 3. é›†æˆæµ‹è¯•æ¡†æ¶ï¼ˆGoogle Testï¼‰

**æ–°å¢æ–‡ä»¶ï¼š**
- `tests/test_tensorrt.cpp`
- `tests/test_scheduler.cpp`
- `tests/test_profiler.cpp`

**CMakeLists.txt æ›´æ–°ï¼š**
```cmake
# tests/CMakeLists.txt
include(FetchContent)
FetchContent_Declare(
  googletest
  GIT_REPOSITORY https://github.com/google/googletest.git
  GIT_TAG release-1.12.1
)
FetchContent_MakeAvailable(googletest)

enable_testing()

add_executable(test_tensorrt test_tensorrt.cpp)
target_link_libraries(test_tensorrt GTest::gtest_main tensorrt_adapter)
gtest_discover_tests(test_tensorrt)
```

**æµ‹è¯•ç¤ºä¾‹ï¼š**
```cpp
#include <gtest/gtest.h>
#include "tensorrt_engine.h"

TEST(TensorRTEngineTest, LoadInvalidEngine) {
    EXPECT_THROW(TensorRTEngine("/nonexistent.engine"), std::runtime_error);
}

TEST(TensorRTEngineTest, InferencePerformance) {
    TensorRTEngine engine("yolov8n.engine");
    auto stats = engine.benchmark(10, 100);
    EXPECT_GT(stats.avg_latency_ms, 0.0f);
    EXPECT_LT(stats.avg_latency_ms, 20.0f);  // æ€§èƒ½å›å½’æ£€æµ‹
}
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 1 å‘¨
**æ”¶ç›Šï¼š** é˜²æ­¢å›å½’ï¼Œæé«˜ä»£ç è´¨é‡

---

### 4. åˆ›å»ºç¼ºå¤±çš„æ–‡æ¡£

**éœ€è¦åˆ›å»ºçš„æ–‡æ¡£ï¼š**

#### 4.1 `docs/api_reference.md`
```markdown
# API Reference

## REST API Endpoints

### POST /api/inference
**Description:** Submit inference task

**Request:**
```json
{
  "model": "yolov8",
  "input_data": [1.0, 2.0, ...],
  "priority": "HIGH"
}
```

**Response:**
```json
{
  "task_id": "abc123",
  "status": "queued",
  "estimated_latency_ms": 8.72
}
```
```

#### 4.2 `docs/architecture.md`
- ç³»ç»Ÿæ¶æ„å›¾è¯¦è§£
- å„æ¨¡å—èŒè´£
- æ•°æ®æµå›¾

#### 4.3 `docs/custom_kernels.md`
- CUDA å†…æ ¸å¼€å‘æŒ‡å—
- æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- Tile å¤§å°é€‰æ‹©ç­–ç•¥

#### 4.4 `docs/troubleshooting.md`ï¼ˆæ–°å¢ï¼‰
- å¸¸è§ç¼–è¯‘é”™è¯¯
- è¿è¡Œæ—¶é—®é¢˜è¯Šæ–­
- æ€§èƒ½é—®é¢˜æ’æŸ¥

**é¢„ä¼°å·¥ä½œé‡ï¼š** 2-3 å¤©
**æ”¶ç›Šï¼š** é™ä½ä½¿ç”¨é—¨æ§›ï¼Œå‡å°‘æ”¯æŒè´Ÿæ‹…

---

## ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆçŸ­æœŸæ”¹è¿›ï¼‰

### 5. å®ç°çœŸå®çš„ Profiler æŒ‡æ ‡

**å—å½±å“æ–‡ä»¶ï¼š**
- `core/profiler/profiler.cpp:111-132`

**é›†æˆ NVMLï¼š**
```cpp
#include <nvml.h>

void Profiler::collectMetrics() {
    nvmlInit();

    nvmlDevice_t device;
    nvmlDeviceGetHandleByIndex(0, &device);

    // SM åˆ©ç”¨ç‡
    nvmlUtilization_t utilization;
    nvmlDeviceGetUtilizationRates(device, &utilization);
    metrics.sm_utilization = utilization.gpu / 100.0f;

    // æ¸©åº¦
    unsigned int temp;
    nvmlDeviceGetTemperature(device, NVML_TEMPERATURE_GPU, &temp);
    metrics.temperature_celsius = static_cast<float>(temp);

    // åŠŸè€—
    unsigned int power;
    nvmlDeviceGetPowerUsage(device, &power);
    metrics.power_usage_watts = power / 1000.0f;

    nvmlShutdown();
}
```

**CMakeLists.txt æ›´æ–°ï¼š**
```cmake
find_library(NVML_LIBRARY nvidia-ml HINTS /usr/lib/aarch64-linux-gnu)
target_link_libraries(profiler ${NVML_LIBRARY})
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 4-6 å°æ—¶
**æ”¶ç›Šï¼š** çœŸå®æ€§èƒ½ç›‘æ§æ•°æ®

---

### 6. å®ç°åŠ¨æ€æ‰¹å¤„ç†

**å—å½±å“æ–‡ä»¶ï¼š**
- `core/scheduler/scheduler.cpp:272-298`

**å½“å‰é—®é¢˜ï¼š** `tryBatchTasks()` å‡½æ•°å­˜åœ¨ä½†ä»æœªè¢«è°ƒç”¨

**ä¿®å¤æ–¹æ¡ˆï¼š**
```cpp
// scheduler.cpp:100 é™„è¿‘
void InferenceScheduler::workerThread(int worker_id) {
    while (running_) {
        std::vector<InferenceTask> batch;

        {
            std::unique_lock<std::mutex> lock(queue_mutex_);
            queue_cv_.wait(lock, [this] {
                return !task_queue_.empty() || !running_;
            });

            if (!running_) break;

            // å°è¯•æ‰¹å¤„ç†
            InferenceTask first = task_queue_.top();
            batch = tryBatchTasks(first);  // âœ… å®é™…è°ƒç”¨

            // ä»é˜Ÿåˆ—ç§»é™¤æ‰¹å¤„ç†çš„ä»»åŠ¡
            for (const auto& task : batch) {
                task_queue_.pop();
            }
        }

        // æ‰§è¡Œæ‰¹é‡æ¨ç†
        executeBatch(batch, worker_id);
    }
}
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 1-2 å¤©
**æ”¶ç›Šï¼š** æå‡ååé‡ 20-40%

---

### 7. æ¶ˆé™¤ç¡¬ç¼–ç å€¼

**å—å½±å“æ–‡ä»¶ï¼š**
- `CMakeLists.txt:30`
- `README.md:149-154`
- `kernels/optimized/kernels.cu:27`

**ä¿®å¤æ–¹æ¡ˆï¼š**

**7.1 CUDA æ¶æ„**
```cmake
# CMakeLists.txt
option(CUDA_ARCHITECTURES "Target CUDA architectures" "87;75;72;61")
set(CMAKE_CUDA_ARCHITECTURES ${CUDA_ARCHITECTURES})
```

**7.2 ç¼–è¯‘å¹¶è¡Œæ•°**
```cmake
# æ·»åŠ åˆ° README.md
CORES=$(nproc)
PARALLEL=$((CORES > 2 ? CORES - 2 : CORES))
make -j${PARALLEL}
```

**7.3 Tile å¤§å°å¯é…ç½®**
```cpp
// kernels.h
cudaError_t gemmFloatOptimized(
    const float* A, const float* B, float* C,
    int M, int N, int K,
    int tile_size = 16,  // æ–°å¢å‚æ•°
    cudaStream_t stream = 0
);
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 4 å°æ—¶
**æ”¶ç›Šï¼š** æé«˜å¯ç§»æ¤æ€§å’Œçµæ´»æ€§

---

## ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸæ”¹è¿›ï¼‰

### 8. å®ç°å†…å­˜æ± 

**æ–°å¢æ–‡ä»¶ï¼š**
- `core/memory_pool/memory_pool.h`
- `core/memory_pool/memory_pool.cpp`

**æ¥å£è®¾è®¡ï¼š**
```cpp
class GPUMemoryPool {
public:
    void* allocate(size_t bytes);
    void deallocate(void* ptr);
    void defragment();

private:
    std::map<size_t, std::vector<void*>> free_blocks_;
    std::unordered_map<void*, size_t> allocated_blocks_;
};
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 1 å‘¨
**æ”¶ç›Šï¼š** å‡å°‘å†…å­˜ç¢ç‰‡ï¼Œæå‡åˆ†é…æ€§èƒ½

---

### 9. å®ç° ONNX Runtime é€‚é…å™¨

**æ–°å¢æ–‡ä»¶ï¼š**
- `engines/onnx_adapter/onnx_engine.h`
- `engines/onnx_adapter/onnx_engine.cpp`

**ä¾èµ–ï¼š**
```bash
# å®‰è£… ONNX Runtime
wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.3/onnxruntime-linux-aarch64-1.16.3.tgz
tar -xzf onnxruntime-linux-aarch64-1.16.3.tgz
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 3-5 å¤©
**æ”¶ç›Šï¼š** æ”¯æŒæ›´å¤šæ¨¡å‹æ ¼å¼

---

### 10. æ€§èƒ½ä¼˜åŒ–

**10.1 ä½¿ç”¨ CUDA Events è¿›è¡Œç²¾ç¡®è®¡æ—¶**
```cpp
// tensorrt_engine.cpp:257
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

for (int i = 0; i < iterations; i++) {
    cudaEventRecord(start);
    context_->enqueueV3(stream);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    latencies.push_back(ms);
}
```

**10.2 ä¼˜åŒ–ç»Ÿè®¡æ”¶é›†**
```cpp
// scheduler.cpp:254
// ä½¿ç”¨ circular buffer æ›¿ä»£ vector
#include <boost/circular_buffer.hpp>
boost::circular_buffer<double> queue_wait_times_(1000);
```

**é¢„ä¼°å·¥ä½œé‡ï¼š** 1-2 å¤©
**æ”¶ç›Šï¼š** æ›´å‡†ç¡®çš„åŸºå‡†æµ‹è¯•

---

## ğŸ“Š æ”¹è¿›ä¼˜å…ˆçº§çŸ©é˜µ

| æ”¹è¿›é¡¹ | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | æ”¶ç›Š | å»ºè®®æ—¶é—´ |
|--------|--------|--------|------|----------|
| CUDA é”™è¯¯å¤„ç† | ğŸ”´ é«˜ | 2-4h | é«˜ | ç«‹å³ |
| Python ç»‘å®š | ğŸ”´ é«˜ | 1-2d | æé«˜ | æœ¬å‘¨ |
| æµ‹è¯•æ¡†æ¶ | ğŸ”´ é«˜ | 1w | é«˜ | 2 å‘¨å†… |
| åˆ›å»ºæ–‡æ¡£ | ğŸ”´ é«˜ | 2-3d | ä¸­ | 2 å‘¨å†… |
| NVML é›†æˆ | ğŸŸ¡ ä¸­ | 4-6h | ä¸­ | 1 ä¸ªæœˆå†… |
| åŠ¨æ€æ‰¹å¤„ç† | ğŸŸ¡ ä¸­ | 1-2d | é«˜ | 1 ä¸ªæœˆå†… |
| æ¶ˆé™¤ç¡¬ç¼–ç  | ğŸŸ¡ ä¸­ | 4h | ä¸­ | 1 ä¸ªæœˆå†… |
| å†…å­˜æ±  | ğŸŸ¢ ä½ | 1w | ä¸­ | 3 ä¸ªæœˆå†… |
| ONNX é€‚é…å™¨ | ğŸŸ¢ ä½ | 3-5d | ä½ | æŒ‰éœ€ |
| æ€§èƒ½ä¼˜åŒ– | ğŸŸ¢ ä½ | 1-2d | ä½ | æŒ‰éœ€ |

---

## ğŸ¯ å»ºè®®çš„å®æ–½é¡ºåº

### ç¬¬ 1 å‘¨ï¼šç”Ÿäº§ç¨³å®šæ€§
1. âœ… æ·»åŠ æ‰€æœ‰ CUDA é”™è¯¯å¤„ç†
2. âœ… ä¿®å¤å†…å­˜æ³„æ¼é£é™©
3. âœ… æ”¹è¿›é”™è¯¯æ¶ˆæ¯

### ç¬¬ 2-3 å‘¨ï¼šAPI å¯ç”¨æ€§
1. âœ… å®ç° Python ç»‘å®šï¼ˆpybind11ï¼‰
2. âœ… æµ‹è¯• API ä¸ C++ åç«¯é›†æˆ
3. âœ… æ·»åŠ  API æ–‡æ¡£

### ç¬¬ 4-5 å‘¨ï¼šæµ‹è¯•ä¸æ–‡æ¡£
1. âœ… é›†æˆ Google Test
2. âœ… ç¼–å†™ 20+ å•å…ƒæµ‹è¯•
3. âœ… åˆ›å»ºç¼ºå¤±æ–‡æ¡£

### ç¬¬ 6-8 å‘¨ï¼šåŠŸèƒ½å®Œå–„
1. âœ… å®ç°åŠ¨æ€æ‰¹å¤„ç†
2. âœ… é›†æˆ NVML
3. âœ… æ¶ˆé™¤ç¡¬ç¼–ç å€¼

### æœªæ¥ï¼ˆå¯é€‰ï¼‰
- å†…å­˜æ± å®ç°
- ONNX é€‚é…å™¨
- å¤š GPU æ”¯æŒ
- åˆ†å¸ƒå¼æ¨ç†

---

## ğŸ“ å¿«é€Ÿè¡ŒåŠ¨æ¸…å•ï¼ˆæœ¬å‘¨å¯å®Œæˆï¼‰

**ä»Šå¤©ï¼ˆ2 å°æ—¶ï¼‰ï¼š**
- [ ] ä¸º `tensorrt_engine.cpp:107` æ·»åŠ é”™è¯¯æ£€æŸ¥
- [ ] ä¸º `cuda_hook.cpp:57` æ·»åŠ é”™è¯¯æ£€æŸ¥
- [ ] æ›´æ–° `quick_start.md` ç§»é™¤æŸåé“¾æ¥

**æ˜å¤©ï¼ˆ4 å°æ—¶ï¼‰ï¼š**
- [ ] åˆ›å»º `docs/api_reference.md`
- [ ] åˆ›å»º `docs/troubleshooting.md`
- [ ] æ·»åŠ  `.clang-format` ä»£ç æ ¼å¼åŒ–é…ç½®

**æœ¬å‘¨æœ«ï¼ˆ8 å°æ—¶ï¼‰ï¼š**
- [ ] é›†æˆ pybind11
- [ ] åˆ›å»º Python ç»‘å®šç¤ºä¾‹
- [ ] æµ‹è¯• API å®é™…æ¨ç†

---

## ğŸ”§ å·¥å…·å’Œèµ„æº

**æ¨èå·¥å…·ï¼š**
- `clang-tidy` - é™æ€ä»£ç åˆ†æ
- `valgrind` - å†…å­˜æ³„æ¼æ£€æµ‹
- `nsight-systems` - æ€§èƒ½åˆ†æ
- `cppcheck` - ä»£ç è´¨é‡æ£€æŸ¥

**å‚è€ƒæ–‡æ¡£ï¼š**
- pybind11: https://pybind11.readthedocs.io/
- Google Test: https://google.github.io/googletest/
- NVML API: https://docs.nvidia.com/deploy/nvml-api/
- TensorRT Best Practices: https://docs.nvidia.com/deeplearning/tensorrt/

---

## ğŸ“ˆ é¢„æœŸæˆæœ

å®Œæˆæ‰€æœ‰é«˜ä¼˜å…ˆçº§æ”¹è¿›åï¼š

**ä»£ç è´¨é‡ï¼š**
- âœ… ç”Ÿäº§çº§é”™è¯¯å¤„ç†
- âœ… 80%+ æµ‹è¯•è¦†ç›–ç‡
- âœ… å®Œæ•´æ–‡æ¡£

**åŠŸèƒ½å®Œæ•´æ€§ï¼š**
- âœ… API å®é™…å¯ç”¨ï¼ˆéæ¼”ç¤ºï¼‰
- âœ… çœŸå®æ€§èƒ½ç›‘æ§
- âœ… åŠ¨æ€æ‰¹å¤„ç†å·¥ä½œ

**ç”¨æˆ·ä½“éªŒï¼š**
- âœ… æ¸…æ™°çš„é”™è¯¯æ¶ˆæ¯
- âœ… å®Œæ•´çš„ä½¿ç”¨æ–‡æ¡£
- âœ… æ˜“äºéƒ¨ç½²

**ç®€å†ä»·å€¼ï¼š**
- âœ… ç”Ÿäº§çº§ä»£ç è´¨é‡
- âœ… å®Œæ•´çš„æµ‹è¯•æ¡†æ¶
- âœ… ç«¯åˆ°ç«¯å¯ç”¨ç³»ç»Ÿ

---

**åˆ›å»ºæ—¶é—´ï¼š** 2025-11-17
**ç»´æŠ¤è€…ï¼š** Geoffrey
**çŠ¶æ€ï¼š** æ´»è·ƒå¼€å‘ä¸­
