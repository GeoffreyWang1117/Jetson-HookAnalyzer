# Project Structure

## Overview

This document describes the organization of the HookAnalyzer project.

## Directory Layout

```
HookAnalyzer/
â”œâ”€â”€ README.md                    # English documentation (main)
â”œâ”€â”€ README_CN.md                 # Chinese documentation
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ CMakeLists.txt              # Root CMake configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ CONTRIBUTING.md             # Contribution guidelines
â”œâ”€â”€ DEPLOYMENT_GUIDE.md         # Deployment instructions
â”œâ”€â”€ PROJECT_STRUCTURE.md        # This file
â”‚
â”œâ”€â”€ core/                       # Core framework components
â”‚   â”œâ”€â”€ cuda_hook/             # CUDA API interception layer
â”‚   â”‚   â”œâ”€â”€ cuda_hook.h
â”‚   â”‚   â”œâ”€â”€ cuda_hook.cpp
â”‚   â”‚   â””â”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ scheduler/             # Multi-model inference scheduler
â”‚   â”‚   â”œâ”€â”€ scheduler.h
â”‚   â”‚   â”œâ”€â”€ scheduler.cpp
â”‚   â”‚   â””â”€â”€ CMakeLists.txt
â”‚   â””â”€â”€ profiler/              # Performance profiling utilities
â”‚       â”œâ”€â”€ profiler.h
â”‚       â”œâ”€â”€ profiler.cpp
â”‚       â””â”€â”€ CMakeLists.txt
â”‚
â”œâ”€â”€ engines/                    # Inference engine adapters
â”‚   â”œâ”€â”€ tensorrt_adapter/      # TensorRT C++ wrapper
â”‚   â”‚   â”œâ”€â”€ tensorrt_engine.h
â”‚   â”‚   â”œâ”€â”€ tensorrt_engine.cpp (~350 LOC)
â”‚   â”‚   â””â”€â”€ CMakeLists.txt
â”‚   â””â”€â”€ onnx_adapter/          # ONNX Runtime wrapper (planned)
â”‚       â””â”€â”€ CMakeLists.txt
â”‚
â”œâ”€â”€ kernels/                    # Custom CUDA kernels
â”‚   â””â”€â”€ optimized/             # Optimized kernel implementations
â”‚       â”œâ”€â”€ kernels.h
â”‚       â”œâ”€â”€ kernels.cu         # GEMM, element-wise ops, activations
â”‚       â”œâ”€â”€ gemm_optimized_v2.cu  # Advanced GEMM variants
â”‚       â””â”€â”€ CMakeLists.txt
â”‚
â”œâ”€â”€ benchmarks/                 # Performance benchmarking tools
â”‚   â”œâ”€â”€ benchmark_kernels.cpp
â”‚   â”œâ”€â”€ gemm_analysis.cpp      # GEMM performance analysis
â”‚   â”œâ”€â”€ gemm_compare.cpp       # GEMM variant comparison
â”‚   â””â”€â”€ CMakeLists.txt
â”‚
â”œâ”€â”€ examples/                   # Usage examples and tests
â”‚   â”œâ”€â”€ simple_demo_minimal.cpp
â”‚   â”œâ”€â”€ kernel_test.cpp        # Kernel validation suite
â”‚   â”œâ”€â”€ test_tensorrt.cpp      # TensorRT inference test
â”‚   â””â”€â”€ CMakeLists.txt
â”‚
â”œâ”€â”€ scripts/                    # Build and deployment scripts
â”‚   â”œâ”€â”€ setup_yolov8.py        # YOLOv8 model setup
â”‚   â”œâ”€â”€ test_yolov8_simple.py
â”‚   â”œâ”€â”€ test_yolov8_inference.py
â”‚   â”œâ”€â”€ record_video.sh        # Demo recording
â”‚   â””â”€â”€ convert_to_video.sh
â”‚
â”œâ”€â”€ api/                        # RESTful API service
â”‚   â””â”€â”€ server/
â”‚       â”œâ”€â”€ main.py            # FastAPI application
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ models.py
â”‚       â””â”€â”€ routes/
â”‚
â”œâ”€â”€ monitoring/                 # Metrics and monitoring
â”‚   â”œâ”€â”€ metrics/               # Prometheus exporter
â”‚   â””â”€â”€ dashboard/             # Grafana configurations
â”‚
â”œâ”€â”€ tests/                      # Unit and integration tests
â”‚   â”œâ”€â”€ test_scheduler.cpp
â”‚   â”œâ”€â”€ test_profiler.cpp
â”‚   â””â”€â”€ CMakeLists.txt
â”‚
â”œâ”€â”€ docker/                     # Docker configurations
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ experiments/           # Experimental reports
â”‚   â”‚   â”œâ”€â”€ EXPERIMENT1_REPORT.md       # GEMM optimization analysis
â”‚   â”‚   â”œâ”€â”€ EXPERIMENT3_PROGRESS.md     # Experiment 3 progress log
â”‚   â”‚   â”œâ”€â”€ EXPERIMENT3_RESULTS.md      # TensorRT integration results
â”‚   â”‚   â”œâ”€â”€ EXPERIMENT_ROADMAP.md       # Future experiments
â”‚   â”‚   â”œâ”€â”€ VERIFICATION_REPORT.md      # Initial validation
â”‚   â”‚   â”œâ”€â”€ FINAL_SUMMARY.md           # Project completion summary
â”‚   â”‚   â””â”€â”€ VIDEO_RECORDING_GUIDE.md   # Video demo guide
â”‚   â”œâ”€â”€ media/                 # Media files
â”‚   â”‚   â”œâ”€â”€ hookanalyzer_demo.mp4      # Demo video
â”‚   â”‚   â”œâ”€â”€ demo_colored.png           # Terminal screenshot
â”‚   â”‚   â””â”€â”€ demo_output.txt            # Sample output
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md    # High-level overview
â”‚   â”œâ”€â”€ DEMO_SAMPLES.md        # Demo samples
â”‚   â”œâ”€â”€ quick_start.md         # Quick start guide
â”‚   â””â”€â”€ README_WITH_VIDEO.md   # Documentation with video
â”‚
â”œâ”€â”€ models/                     # Model files (not in repo)
â”œâ”€â”€ logs/                       # Runtime logs
â””â”€â”€ data/                       # Sample data

```

## Key Components

### Core Framework (core/)

**CUDA Hook Layer** (`cuda_hook/`)
- Intercepts CUDA API calls (cudaMalloc, cudaFree, etc.)
- Tracks GPU memory allocations and deallocations
- Provides performance profiling hooks

**Scheduler** (`scheduler/`)
- Priority-based multi-model task scheduling
- Dynamic batching support
- CUDA stream management
- Resource allocation and contention handling

**Profiler** (`profiler/`)
- Real-time performance metrics collection
- GPU utilization tracking
- Latency and throughput measurement

### Inference Engines (engines/)

**TensorRT Adapter** (`tensorrt_adapter/`)
- Complete C++ TensorRT wrapper (~350 LOC)
- Engine loading and serialization
- Synchronous and asynchronous inference
- FP16 precision optimization
- Comprehensive benchmarking capabilities
- **Status:** âœ… Production-ready

**ONNX Adapter** (`onnx_adapter/`)
- **Status:** ðŸ“‹ Planned for future implementation

### Custom Kernels (kernels/optimized/)

**Implemented Kernels:**
- GEMM (General Matrix Multiply)
  - 16Ã—16 tiled implementation (100% occupancy on Jetson)
  - 32Ã—32, 64Ã—64 variants for comparison
  - Double-buffered variant
- Element-wise operations (add, multiply, etc.)
- Activation functions (ReLU, Sigmoid)
- **Performance:** 146 GFLOPS (68.6% of cuBLAS on 512Ã—512 matrices)

### Benchmarking Tools (benchmarks/)

- `benchmark_kernels.cpp` - Comprehensive kernel performance suite
- `gemm_analysis.cpp` - GEMM optimization analysis
- `gemm_compare.cpp` - Compare different GEMM variants

### Documentation (docs/)

**Experimental Reports** (`experiments/`)
- Detailed reports for completed experiments
- Performance data and analysis
- Optimization insights and lessons learned

**Media Files** (`media/`)
- Demo videos
- Screenshots
- Sample outputs

## Build Artifacts

After building with CMake, the `build/` directory contains:

```
build/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ libhook_analyzer.so
â”‚   â”œâ”€â”€ libcuda_hook.so
â”‚   â””â”€â”€ ...
â”œâ”€â”€ kernels/
â”‚   â””â”€â”€ liboptimized_kernels.so
â”œâ”€â”€ engines/
â”‚   â””â”€â”€ libtensorrt_adapter.so
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ kernel_test
â”‚   â”œâ”€â”€ simple_demo
â”‚   â””â”€â”€ test_tensorrt
â””â”€â”€ benchmarks/
    â”œâ”€â”€ benchmark_kernels
    â”œâ”€â”€ gemm_analysis
    â””â”€â”€ gemm_compare
```

## File Naming Conventions

- **Headers:** `*.h` (C++ headers)
- **Source:** `*.cpp` (C++ source), `*.cu` (CUDA source)
- **Documentation:** `*.md` (Markdown)
- **Scripts:** `*.py` (Python), `*.sh` (Shell)
- **Configs:** `*.txt`, `*.yml`, `*.json`

## Important Files

### Configuration
- `CMakeLists.txt` - Build system configuration
- `requirements.txt` - Python dependencies

### Documentation
- `README.md` / `README_CN.md` - Main documentation (English/Chinese)
- `docs/experiments/EXPERIMENT3_RESULTS.md` - YOLOv8 TensorRT results
- `docs/experiments/EXPERIMENT1_REPORT.md` - GEMM optimization analysis

### Media
- `docs/media/hookanalyzer_demo.mp4` - Project demonstration video

## Git Ignored Files

The following are excluded from version control (see `.gitignore`):

- `build/` - CMake build artifacts
- `models/*.engine` - TensorRT engine files
- `models/*.onnx` - ONNX model files
- `logs/*.log` - Runtime logs
- `*.pyc`, `__pycache__/` - Python bytecode
- `.vscode/`, `.idea/` - IDE configurations

## Development Workflow

1. **Core Development:** Modify files in `core/`, `engines/`, `kernels/`
2. **Build:** Use CMake in `build/` directory
3. **Test:** Run executables from `build/examples/` or `build/benchmarks/`
4. **Document:** Update relevant `.md` files in `docs/`
5. **Commit:** Follow conventional commit messages

## Notes

- All C++ source files use C++17 standard
- CUDA files compiled with nvcc (CUDA 12.6)
- TensorRT version: 10.3.0
- Platform: Jetson Orin Nano (Ampere SM 8.7)

---

**Last Updated:** 2025-11-17
**Maintainer:** Geoffrey
